{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell [0]\n",
    "\n",
    "# Standard Libraries\n",
    "import os  # For file system operations such as creating directories.\n",
    "import warnings  # For controlling warning messages.\n",
    "from datetime import timedelta  # For handling time-related operations.\n",
    "\n",
    "# Third-party Libraries\n",
    "\n",
    "# Joblib for saving/loading models\n",
    "import joblib  # For serializing Python objects (like models).\n",
    "\n",
    "# Numpy and Pandas for numerical and data manipulation\n",
    "import numpy as np  # For numerical computations, especially with arrays.\n",
    "import pandas as pd  # For data manipulation and dataframes.\n",
    "\n",
    "# Seaborn and Matplotlib for visualizations\n",
    "import seaborn as sns  # Advanced data visualization library based on matplotlib.\n",
    "import matplotlib.pyplot as plt  # Basic plotting library.\n",
    "\n",
    "# Scipy for statistical distributions (used in RandomizedSearchCV)\n",
    "from scipy.stats import randint, uniform, loguniform  # For specifying parameter search distributions.\n",
    "\n",
    "# Sklearn utilities and transformers\n",
    "from sklearn.base import BaseEstimator, TransformerMixin  # Base classes for creating custom transformers/estimators.\n",
    "from sklearn.compose import ColumnTransformer  # For applying different preprocessing pipelines to specific columns.\n",
    "from sklearn.pipeline import Pipeline, make_pipeline  # For creating machine learning workflows.\n",
    "\n",
    "# Sklearn preprocessors and imputers\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler  # For encoding, feature generation, and scaling.\n",
    "from sklearn.impute import SimpleImputer  # For handling missing values in datasets.\n",
    "\n",
    "# Sklearn model evaluation and hyperparameter tuning\n",
    "from sklearn.model_selection import (KFold, RandomizedSearchCV, cross_val_predict, cross_val_score, train_test_split)  # For cross-validation, hyperparameter tuning, and splitting datasets.\n",
    "from sklearn.metrics import mean_squared_error  # For evaluating model performance with error metrics like MSE.\n",
    "\n",
    "# Sklearn models\n",
    "from sklearn.linear_model import BayesianRidge, Ridge, LinearRegression, Lasso  # Linear models for regression tasks.\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor  # Ensemble models for regression.\n",
    "from sklearn.svm import SVR  # Support Vector Regression.\n",
    "from sklearn.neural_network import MLPRegressor  # Neural network regressor model.\n",
    "\n",
    "# XGBoost and LightGBM - Gradient Boosting models\n",
    "from xgboost import XGBRegressor  # XGBoost regressor for gradient boosting.\n",
    "from lightgbm import LGBMRegressor  # LightGBM regressor for fast gradient boosting.\n",
    "\n",
    "# Warnings related to model convergence\n",
    "from sklearn.exceptions import ConvergenceWarning  # To suppress warnings related to non-convergence.\n",
    "\n",
    "#Cell [1]\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"A value is trying to be set on a copy of a DataFrame\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"No further splits with positive gain\")\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Create directories for storing outputs\n",
    "os.makedirs('figures', exist_ok=True)  # For storing plots and visualizations\n",
    "os.makedirs('models', exist_ok=True)   # For saving trained models\n",
    "os.makedirs('saved_objects', exist_ok=True)  # For miscellaneous saved objects\n",
    "\n",
    "# Configure KFold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to save models and pipelines\n",
    "def store_model_or_pipeline(obj, name=\"\"):\n",
    "    \"\"\"Store a trained model or pipeline to the 'models' folder.\"\"\"\n",
    "    if name == \"\":\n",
    "        name = type(obj).__name__\n",
    "    joblib.dump(obj, f'models/{name}.pkl')\n",
    "    print(f'{name} saved successfully.')\n",
    "\n",
    "# Function to load saved models and pipelines\n",
    "def load_model_or_pipeline(name):\n",
    "    \"\"\"Load a model or pipeline from the 'models' folder.\"\"\"\n",
    "    obj = joblib.load(f'models/{name}.pkl')\n",
    "    print(f'{name} loaded successfully.')\n",
    "    return obj\n",
    "\n",
    "\n",
    "\n",
    "#Cell [2]\n",
    "\n",
    "# Load the data and perform initial cleaning\n",
    "raw_data = pd.read_csv('datasets/NewYork.csv')\n",
    "raw_data['datetime'] = pd.to_datetime(raw_data['datetime'])\n",
    "raw_data.drop(columns=[\"name\", \"icon\", \"stations\", \"description\"], inplace=True)\n",
    "\n",
    "# Check for missing values before imputation\n",
    "print(\"\\nMissing values before imputation:\")\n",
    "print(raw_data.isnull().sum())\n",
    "\n",
    "# Define function to remove outliers using IQR method\n",
    "def remove_outliers(df, column, factor=1.5):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - factor * IQR\n",
    "    upper_bound = Q3 + factor * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "# Remove outliers from 'tempmax' column\n",
    "raw_data = remove_outliers(raw_data, 'tempmax')\n",
    "print(\"\\nShape of data after removing outliers:\", raw_data.shape)\n",
    "\n",
    "# Impute missing values\n",
    "for column in raw_data.columns:\n",
    "    if raw_data[column].dtype == 'object':\n",
    "        raw_data[column].fillna('Unknown', inplace=True)  # For categorical columns\n",
    "    else:\n",
    "        raw_data[column].fillna(raw_data[column].median(), inplace=True)  # For numerical columns\n",
    "\n",
    "# Check for missing values after imputation\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(raw_data.isnull().sum())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Cell [3]\n",
    "\n",
    "print('\\n____________ Dataset info ____________')\n",
    "print(raw_data.info())              \n",
    "print('\\n____________ Some first data examples ____________')\n",
    "print(raw_data.head(3)) \n",
    "print('\\n____________ Statistics of numeric features ____________')\n",
    "print(raw_data.describe())    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Cell [3.1]\n",
    "\n",
    "# Create correlation matrix for numeric features\n",
    "plt.figure(figsize=(12, 10))\n",
    "numeric_data = raw_data.select_dtypes(include=[np.number])\n",
    "corr = numeric_data.corr()\n",
    "\n",
    "# Handle any non-finite values in the correlation matrix\n",
    "if not np.isfinite(corr.values).all():\n",
    "    print(\"Warning: Correlation matrix contains non-finite values. Replacing with 0.\")\n",
    "    corr_array = np.nan_to_num(corr.values, nan=0, posinf=0, neginf=0)\n",
    "    corr = pd.DataFrame(corr_array, index=corr.index, columns=corr.columns)\n",
    "\n",
    "# Create and display heatmap of the correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True)\n",
    "plt.title('Heatmap of Feature Correlations')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the heatmap as a high-resolution image\n",
    "plt.savefig('figures/correlation_heatmap.png', format='png', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Cell [3.2]\n",
    "\n",
    "# Select numeric features from the dataset\n",
    "numeric_features = raw_data.select_dtypes(include=[np.number]).columns\n",
    "n_features = len(numeric_features)\n",
    "\n",
    "# Calculate the number of rows needed for the subplot grid\n",
    "n_rows = (n_features + 1) // 2\n",
    "\n",
    "# Create a large figure to accommodate all histograms\n",
    "plt.figure(figsize=(15, 5 * n_rows))\n",
    "\n",
    "# Create a histogram for each numeric feature\n",
    "for i, feature in enumerate(numeric_features, 1):\n",
    "    plt.subplot(n_rows, 2, i)\n",
    "    sns.histplot(raw_data[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "\n",
    "# Adjust the layout and save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/hist_raw_data.png', format='png', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Cell [3.3]\n",
    "\n",
    "# Select all numeric features except 'tempmax'\n",
    "numeric_features = [col for col in numeric_features if col != 'tempmax']\n",
    "n_features = len(numeric_features)\n",
    "\n",
    "# Calculate the number of rows needed for the subplot grid\n",
    "n_rows = (n_features + 1) // 2\n",
    "\n",
    "# Create a large figure to accommodate all scatter plots\n",
    "plt.figure(figsize=(15, 5 * n_rows))\n",
    "\n",
    "# Create a scatter plot for each feature vs tempmax\n",
    "for i, feature in enumerate(numeric_features, 1):\n",
    "    plt.subplot(n_rows, 2, i)\n",
    "    sns.scatterplot(data=raw_data, x=feature, y='tempmax', alpha=0.5)\n",
    "    plt.title(f'Max Temperature vs {feature}')\n",
    "\n",
    "# Adjust the layout and save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/scatter_tempmax_vs_features.png', format='png', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Cell [3.4]\n",
    "\n",
    "# Select the temperature-related features for plotting\n",
    "temp_features = ['temp', 'feelslike', 'tempmax', 'feelslikemax', 'tempmin', 'feelslikemin']\n",
    "\n",
    "# Create the boxplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(data=raw_data[temp_features])\n",
    "\n",
    "# Set titles and labels for the plot\n",
    "plt.title('Distribution of Temperature Variables')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.xticks(ticks=range(len(temp_features)), labels=temp_features)\n",
    "\n",
    "# Adjust layout and save the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/boxplot_temp_distributions.png', format='png', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Cell [4]\n",
    "class EnhancedFeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_features=True):\n",
    "        self.add_features = add_features\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    ## Time-based Features\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        if isinstance(X_, pd.DataFrame) and self.add_features and 'datetime' in X_.columns:\n",
    "            # Add new features from datetime\n",
    "            X_['day_of_year'] = X_['datetime'].dt.dayofyear\n",
    "            X_['month'] = X_['datetime'].dt.month\n",
    "            X_['day_of_week'] = X_['datetime'].dt.dayofweek\n",
    "            X_['is_weekend'] = X_['day_of_week'].isin([5, 6]).astype(int)\n",
    "            X_['day_of_year_sin'] = np.sin(2 * np.pi * X_['day_of_year'] / 365.25)\n",
    "            X_['day_of_year_cos'] = np.cos(2 * np.pi * X_['day_of_year'] / 365.25)\n",
    "            X_.drop(columns=['datetime'], inplace=True)\n",
    "        return X_\n",
    "    \n",
    "    \n",
    "    \n",
    "### Lag Features and Rolling Averages\n",
    "# Function to add lag features\n",
    "def add_lag_features(df, column, lags):\n",
    "    for lag in lags:\n",
    "        df[f'{column}_lag_{lag}'] = df[column].shift(lag)\n",
    "    return df\n",
    "\n",
    "# Function to add rolling averages\n",
    "def add_rolling_features(df, column, windows):\n",
    "    for window in windows:\n",
    "        df[f'{column}_rolling_{window}'] = df[column].rolling(window).mean()\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "## Feature Selection and Correlation Analysis\n",
    "# Apply lag features and rolling averages to selected columns\n",
    "selected_columns = ['tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike']\n",
    "for col in selected_columns:\n",
    "    raw_data = add_lag_features(raw_data, col, lags=[1, 2, 3])\n",
    "    raw_data = add_rolling_features(raw_data, col, windows=[3, 7])\n",
    "\n",
    "# Evaluate Feature Correlation\n",
    "numeric_data = raw_data.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_data.corr()\n",
    "tempmax_correlation = correlation_matrix['tempmax'].sort_values(ascending=False)\n",
    "print(\"Correlation of features with tempmax:\\n\", tempmax_correlation)\n",
    "\n",
    "# Select features based on correlation threshold\n",
    "correlation_threshold = 0.8\n",
    "selected_features = tempmax_correlation[abs(tempmax_correlation) > correlation_threshold].index.tolist()\n",
    "selected_features.remove('tempmax')  # Remove target variable from features\n",
    "\n",
    "# Exclude specific features\n",
    "excluded_features = ['solarenergy', 'solarradiation', 'uvindex']\n",
    "selected_features = [feature for feature in selected_features if feature not in excluded_features]\n",
    "\n",
    "print(f\"Selected features based on correlation (threshold = {correlation_threshold}):\\n\", selected_features)\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = [feature for feature in selected_features if raw_data[feature].dtype in [np.float64, np.int64]]\n",
    "categorical_features = [feature for feature in selected_features if raw_data[feature].dtype == 'object']\n",
    "\n",
    "# Define transformers for numeric and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Full pipeline with EnhancedFeatureAdder and preprocessor\n",
    "enhanced_pipeline = Pipeline(steps=[\n",
    "    ('feature_adder', EnhancedFeatureAdder()),\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "# Save the full pipeline\n",
    "store_model_or_pipeline(enhanced_pipeline, name=\"full_pipeline\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Cell [5]\n",
    "\n",
    "# Prepare the data for modeling\n",
    "X = raw_data.drop('tempmax', axis=1)  # Features\n",
    "y = raw_data['tempmax']  # Target variable\n",
    "X_processed = enhanced_pipeline.fit_transform(X, y)  # Apply our preprocessing pipeline\n",
    "\n",
    "# Split the data into training and test sets\n",
    "# We use 80% for training and 20% for testing, which is a common split ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "## Model Implementation and Evaluation\n",
    "# Define a diverse set of models to evaluate\n",
    "models = {\n",
    "    'RandomForestReg': RandomForestRegressor(random_state=42),\n",
    "    'GradientBoostingReg': GradientBoostingRegressor(random_state=42),\n",
    "    'LGBMReg': LGBMRegressor(random_state=42, verbosity=-1),\n",
    "    'XGBBoost': XGBRegressor(random_state=42),\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    'LinearReg': LinearRegression(),\n",
    "    'Ridge': Ridge(random_state=42),\n",
    "    'Lasso': Lasso(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'MLPRegressor': MLPRegressor(random_state=42),\n",
    "    'PolynomialReg': make_pipeline(PolynomialFeatures(), LinearRegression())\n",
    "}\n",
    "\n",
    "# Justification for model selection:\n",
    "# 1. Tree-based models (RandomForest, GradientBoosting, LightGBM, XGBoost): Good for capturing non-linear relationships and interactions\n",
    "# 2. Linear models (LinearReg, Ridge, Lasso): Simple, interpretable, and work well if relationships are mostly linear\n",
    "# 3. BayesianRidge: Combines linear regression with Bayesian inference, good for uncertainty estimation\n",
    "# 4. SVR: Can capture non-linear relationships using kernel tricks\n",
    "# 5. MLPRegressor: Neural network that can capture complex patterns\n",
    "# 6. PolynomialReg: Can capture non-linear relationships by adding polynomial features\n",
    "\n",
    "### K-Fold Cross-validation\n",
    "best_model_name = None\n",
    "best_cross_val_rmse = float('inf')\n",
    "\n",
    "print('\\n____________ K-Fold Cross Validation and Residual Analysis ____________')\n",
    "for name, model in models.items():\n",
    "    model_pipeline = Pipeline(steps=[('model', model)])\n",
    "    \n",
    "    # Perform K-Fold Cross-validation\n",
    "    # We use 5-fold CV as it provides a good balance between bias and variance in error estimation\n",
    "    cv_rmse_scores = -cross_val_score(model_pipeline, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
    "    avg_rmse = np.sqrt(cv_rmse_scores.mean())\n",
    "    \n",
    "    # Save the RMSE scores for each fold\n",
    "    joblib.dump(cv_rmse_scores, f'saved_objects/{name}_rmse.pkl')\n",
    "    \n",
    "    print(f'{name:<20} K-Fold Avg RMSE: {avg_rmse:.4f}')\n",
    "    \n",
    "    \n",
    "    ### Residual Analysis\n",
    "    # Calculate and plot residuals\n",
    "    y_train_pred = cross_val_predict(model_pipeline, X_train, y_train, cv=kfold)\n",
    "    residuals = y_train - y_train_pred\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(residuals, kde=True, bins=50)\n",
    "    plt.title(f'Residuals Distribution - {name}')\n",
    "    plt.savefig(f'figures/residuals_{name}.png', format='png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Update best model if current model performs better\n",
    "    if avg_rmse < best_cross_val_rmse:\n",
    "        best_cross_val_rmse = avg_rmse\n",
    "        best_model_name = name\n",
    "\n",
    "# Print and store the best model\n",
    "print(f\"\\nBest model after K-Fold Cross Validation: {best_model_name} with RMSE: {best_cross_val_rmse:.4f}\")\n",
    "best_model = models[best_model_name]\n",
    "store_model_or_pipeline(best_model, name=best_model_name)\n",
    "\n",
    "# Display residual plot for the best model\n",
    "model_pipeline = Pipeline(steps=[('model', best_model)])\n",
    "y_train_pred_best = cross_val_predict(model_pipeline, X_train, y_train, cv=kfold)\n",
    "residuals_best = y_train - y_train_pred_best\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals_best, kde=True, bins=50)\n",
    "plt.title(f'Residuals Distribution - {best_model_name}')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Justification for using RMSE as the primary metric:\n",
    "# 1. RMSE is in the same unit as the target variable (temperature), making it interpretable\n",
    "# 2. It penalizes large errors more heavily, which is important for temperature prediction\n",
    "# 3. It's widely used in regression problems, allowing for easy comparison with other models or studies\n",
    "\n",
    "# Justification for using K-fold Cross-Validation:\n",
    "# 1. Provides a more robust estimate of model performance by using all data for both training and validation\n",
    "# 2. Helps to detect overfitting by showing how well the model generalizes to unseen data\n",
    "# 3. Reduces the impact of data splitting randomness on model evaluation\n",
    "\n",
    "\n",
    "\n",
    "#Cell [6]\n",
    "\n",
    "print(f'\\n____________ Fine-tuning the best model: {best_model_name} ____________')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Ensure input data is scaled\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "param_grids = {\n",
    "    'RandomForestReg': {\n",
    "        'n_estimators': randint(100, 2000),\n",
    "        'max_depth': randint(5, 50),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 20),\n",
    "        'max_features': uniform(0.1, 0.9)\n",
    "    },\n",
    "    'GradientBoostingReg': {\n",
    "        'n_estimators': randint(100, 2000),\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'max_depth': randint(3, 20),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 20),\n",
    "        'subsample': uniform(0.5, 0.5)\n",
    "    },\n",
    "    'LGBMReg': {\n",
    "        'num_leaves': randint(20, 200),\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'n_estimators': randint(100, 2000),\n",
    "        'min_child_samples': randint(1, 50),\n",
    "        'subsample': uniform(0.5, 0.5),\n",
    "        'colsample_bytree': uniform(0.5, 0.5),\n",
    "        'verbosity': [-1]\n",
    "    },\n",
    "    'XGBBoost': {\n",
    "        'n_estimators': randint(100, 2000),\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'max_depth': randint(3, 20),\n",
    "        'min_child_weight': randint(1, 10),\n",
    "        'subsample': uniform(0.5, 0.5),\n",
    "        'colsample_bytree': uniform(0.5, 0.5),\n",
    "        'gamma': uniform(0, 0.5)\n",
    "    },\n",
    "    'BayesianRidge': {\n",
    "        'alpha_1': uniform(0.001, 1),\n",
    "        'alpha_2': uniform(0.001, 1),\n",
    "        'lambda_1': uniform(0.001, 1),\n",
    "        'lambda_2': uniform(0.001, 1)\n",
    "    },\n",
    "    'LinearReg': {\n",
    "        'fit_intercept': [True, False],\n",
    "        'copy_X': [True, False],\n",
    "        'positive': [True, False]\n",
    "    },\n",
    "    'Ridge': {\n",
    "        'alpha': loguniform(1e-3, 1e2),\n",
    "        'max_iter': [5000, 10000]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': loguniform(1e-3, 1e2),\n",
    "        'max_iter': [5000, 10000]\n",
    "    },\n",
    "    'SVR': {\n",
    "        'C': loguniform(1e-2, 1e2),\n",
    "        'epsilon': loguniform(1e-3, 1),\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,), (100,100), (100,50,100)],\n",
    "        'activation': ['tanh', 'relu', 'logistic'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': loguniform(1e-4, 1e-1),\n",
    "        'learning_rate': ['constant','adaptive'],\n",
    "        'learning_rate_init': loguniform(1e-4, 1e-1),\n",
    "        'max_iter': [200, 500, 1000],\n",
    "        'early_stopping': [True, False],\n",
    "        'momentum': uniform(0.0, 1.0),\n",
    "        'nesterovs_momentum': [True, False]\n",
    "    },\n",
    "    'PolynomialReg': {\n",
    "        'polynomialfeatures__degree': randint(2, 5),\n",
    "        'linearregression__fit_intercept': [True, False]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Fine-tune the best model using RandomizedSearchCV\n",
    "grid_search = RandomizedSearchCV(best_model, param_distributions=param_grids.get(best_model_name, {}),\n",
    "                                 n_iter=100, cv=kfold, \n",
    "                                 scoring='neg_mean_squared_error', n_jobs=-1, \n",
    "                                 random_state=42, verbose=1, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Save the tuned model as 'SOLUTION_model.pkl'\n",
    "best_model_tuned = grid_search.best_estimator_\n",
    "store_model_or_pipeline(best_model_tuned, name='SOLUTION_model')\n",
    "\n",
    "# Save RMSE of the tuned model\n",
    "best_tuned_rmse = np.sqrt(-grid_search.best_score_)\n",
    "joblib.dump(best_tuned_rmse, 'saved_objects/SOLUTION_model_rmse.pkl')\n",
    "print(f\"Best RMSE for {best_model_name} after tuning: {best_tuned_rmse:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "#Cell [7]\n",
    "\n",
    "# ANALYZE AND TEST THE SOLUTION\n",
    "\n",
    "# Print the best RMSE achieved after hyperparameter tuning\n",
    "print(f\"\\nBest RMSE for {best_model_name} after tuning: {np.sqrt(-grid_search.best_score_):.4f}\")\n",
    "\n",
    "# Make predictions on the test set using the tuned model\n",
    "y_pred = best_model_tuned.predict(X_test)\n",
    "\n",
    "# Calculate the RMSE on the test set\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'\\nPerformance on test data: RMSE: {test_rmse:.4f}')\n",
    "\n",
    "# Save the test RMSE for future reference\n",
    "joblib.dump(test_rmse, 'saved_objects/test_rmse.pkl')\n",
    "\n",
    "\n",
    "\n",
    "#Cell [8]\n",
    "\n",
    "# Define a function to find the closest date in the historical data\n",
    "def find_closest_date(target_date, data):\n",
    "    try:\n",
    "        # Try to find the same date in the previous year\n",
    "        target_date = target_date.replace(year=target_date.year - 1)\n",
    "    except ValueError:\n",
    "        # Handle leap year case (February 29)\n",
    "        target_date = target_date.replace(year=target_date.year - 1, day=28)\n",
    "    # Find the closest date in the historical data\n",
    "    closest_date = data['datetime'].iloc[(data['datetime'] - target_date).abs().argsort()[0]]\n",
    "    return data.loc[data['datetime'] == closest_date].iloc[0]\n",
    "\n",
    "# Generate future dates for prediction\n",
    "last_date = raw_data['datetime'].max()\n",
    "future_dates = pd.date_range(start=last_date + timedelta(days=1), periods=200)\n",
    "future_data = pd.DataFrame({'datetime': future_dates})\n",
    "\n",
    "# Fill in other features based on historical data\n",
    "for col in raw_data.columns:\n",
    "    if col not in ['datetime', 'tempmax']:\n",
    "        future_data[col] = future_data['datetime'].apply(lambda x: find_closest_date(x, raw_data)[col])\n",
    "\n",
    "# Process future data using the same pipeline as training data\n",
    "future_processed = enhanced_pipeline.transform(future_data)\n",
    "\n",
    "# Make predictions for future dates\n",
    "future_pred = best_model_tuned.predict(future_processed)\n",
    "future_data['predicted_tempmax'] = future_pred\n",
    "\n",
    "# Save future predictions to a CSV file\n",
    "future_data[['datetime', 'predicted_tempmax']].to_csv('future_predictions_200days.csv', index=False)\n",
    "\n",
    "# Visualize future predictions\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(future_data['datetime'], future_data['predicted_tempmax'], label='Predicted Max Temperature', alpha=0.7)\n",
    "plt.title('Predicted Maximum Temperature for the Next 200 Days')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig('figures/future_predictions_200days_plot.png', format='png', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Print statistics of future predictions\n",
    "print(\"\\nPredicted Max Temperature:\")\n",
    "print(f\"Min: {future_data['predicted_tempmax'].min():.2f}°C\")\n",
    "print(f\"Max: {future_data['predicted_tempmax'].max():.2f}°C\")\n",
    "print(f\"Avg: {future_data['predicted_tempmax'].mean():.2f}°C\")\n",
    "\n",
    "\n",
    "#Cell [9]\n",
    "\n",
    "print(\"\\n____________ CONCLUSION ____________\")\n",
    "print(\"\"\"In this notebook, we have effectively constructed a Machine Learning pipeline that utilizes historical weather data from New York to forecast the maximum temperature (tempmax). \n",
    "      We conducted comprehensive coverage of the entire workflow, encompassing data preprocessing, feature engineering, model training, and hyperparameter tuning:\"\"\")\n",
    "print(f\"\"\"\n",
    "   - The best performing model was: {best_model_name}\n",
    "   - Best model RMSE on test data: {test_rmse:.4f}\n",
    "   Generated predictions for the next 200 days:\n",
    "   - The predicted temperatures range from {future_data['predicted_tempmax'].min():.2f}°C to {future_data['predicted_tempmax'].max():.2f}°C.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nPrediction and analysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
